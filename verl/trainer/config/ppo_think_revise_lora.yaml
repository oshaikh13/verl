# Simple PPO configuration that enables the think/retrieve/revise loop with a LoRA policy.
#
# Usage:
#   python -m verl.trainer.main_ppo \
#     --config-path ../../trainer/config \
#     --config-name ppo_think_revise_lora
#
# A companion launch script is available under examples/ppo_trainer/think_revise_lora/run.sh.

defaults:
  - ppo_trainer

actor_rollout_ref:
  model:
    # Set the base model path via the POLICY_BASE environment variable or edit directly.
    path: ${oc.env:POLICY_BASE,~/models/qwen/Qwen2-7B-Instruct}
    tokenizer_path: ${actor_rollout_ref.model.path}
    lora_rank: 32
    lora_alpha: 16
    target_modules: all-linear

  rollout:
    name: vllm
    tensor_model_parallel_size: 1
    prompt_length: 512
    response_length: 512
    temperature: 0.8
    top_p: 0.9
    val_kwargs:
      temperature: 0.0
      top_p: 1.0
    think_revise:
      enable: true
      share_across_workers: true
      actor_name: verl_think_retriever_default
      retriever_top_k: 8
      memory_top_m: 3
      time_decay_lambda: 0.0005
      dedup_jaccard: 0.9
      n_claims_field: n_claims
      future_len_field: future_len
      timestamp_field: ts
      default_n_claims: 3
      default_future_len: 3

data:
  train_files: ${oc.env:TRAIN_DATA,~/data/rlhf/demo/train.parquet}
  val_files: ${oc.env:VAL_DATA,~/data/rlhf/demo/val.parquet}
  max_prompt_length: ${actor_rollout_ref.rollout.prompt_length}
  return_full_prompt: true
  filter_overlong_prompts: true
  prompt_key: prompt
  train_batch_size: 256

algorithm:
  adv_estimator: gae
  use_kl_in_reward: false
  rollout_is_threshold: null

trainer:
  total_epochs: 1
  nnodes: 1
  n_gpus_per_node: 1
  project_name: think_revise_demo
  experiment_name: ${oc.env:RUN_NAME,think_revise_lora}
  logger: ["console"]
  save_freq: -1
  test_freq: -1

reward_model:
  enable: false
  reward_manager: batch
  reward_kwargs: {}

custom_reward_function:
  path: ${oc.env:REWARD_FN_PATH,examples/ppo_trainer/think_revise_lora/reward.py}
  name: compute_score
  reward_kwargs: {}

ray_kwargs:
  ray_init:
    num_cpus: 8
